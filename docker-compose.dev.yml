# =============================================================================
# Docker Compose for vLLM + FlashInfer Development on DGX Spark
# =============================================================================
#
# Usage:
#   docker compose -f docker-compose.dev.yml up -d    # Start dev container
#   docker compose -f docker-compose.dev.yml exec dev bash  # Shell into container
#   docker compose -f docker-compose.dev.yml down     # Stop container
#
# First run will install FlashInfer and vLLM in editable mode (takes a few minutes).
# Subsequent runs will be instant.
#
# =============================================================================

services:
  dev:
    build:
      context: .
      dockerfile: Dockerfile.dev
    image: vllm-dev:latest
    container_name: vllm-dev
    
    # Use host networking for multi-node Ray/NCCL
    network_mode: host
    
    # GPU access
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    ipc: host
    
    # RDMA device access for NCCL IB/RoCE transport
    devices:
      - /dev/infiniband:/dev/infiniband
    
    # Enable full nsys profiling (CPU samples, context switches)
    # IPC_LOCK required for RDMA memory pinning
    cap_add:
      - SYS_ADMIN
      - IPC_LOCK
    security_opt:
      - seccomp=unconfined
    
    # Keep container running for interactive use
    stdin_open: true
    tty: true
    
    # Resource limits
    ulimits:
      memlock: -1
      stack: 67108864
    
    # No port mapping needed with network_mode: host
    # All ports are directly accessible on host
    
    # Mount source code repos (EDIT THESE PATHS)
    volumes:
      # Source code mounts - your local repos
      - ~/projects/vllm:/workspace/vllm
      - ~/projects/flashinfer:/workspace/flashinfer
      
      # Scripts from this repo
      - ./scripts:/workspace/scripts
      
      # Persistent caches - survive container restarts
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./.cache/vllm:/root/.cache/vllm
      - ./.cache/torch:/root/.cache/torch
      - ./.cache/triton:/root/.cache/triton
      - ./.cache/flashinfer:/root/.cache/flashinfer
      - ./.cache/uv:/root/.cache/uv
      - ./.cache/ccache:/root/.ccache
      
      # Persist pip metadata for faster reinstalls after container restart
      - ./.cache/pip-metadata:/root/.cache/pip
    
    # Environment variables
    environment:
      # CRITICAL: Required for FlashInfer CUTLASS MoE to work with editable install
      - PYTHONPATH=/workspace/flashinfer:/workspace/vllm
      
      # Tiktoken encodings for tokenizer
      - TIKTOKEN_ENCODINGS_BASE=/workspace/tiktoken_encodings
      
      # Optional: HuggingFace token for gated models
      # - HF_TOKEN=${HF_TOKEN}
      
      # NOTE: vLLM runtime configuration (attention backends, MoE kernels, etc.)
      # should be set explicitly when starting the server, not as default env vars.
      # This allows flexibility in testing different configurations.
      # Example: VLLM_MXFP4_MOE_KERNEL=marlin vllm serve ...
    
    # Default: start a shell
    command: /bin/bash
    
    # Working directory
    working_dir: /workspace

  # Optional: Run vLLM server as a separate service
  # Start with: docker compose -f docker-compose.dev.yml up serve
  serve:
    profiles:
      - serve
    build:
      context: .
      dockerfile: Dockerfile.dev
    image: vllm-dev:latest
    container_name: vllm-serve
    
    # Use host networking for multi-node Ray/NCCL
    network_mode: host
    
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    ipc: host
    
    # RDMA device access for NCCL IB/RoCE transport
    devices:
      - /dev/infiniband:/dev/infiniband
    
    # IPC_LOCK required for RDMA memory pinning
    cap_add:
      - IPC_LOCK
    
    ulimits:
      memlock: -1
      stack: 67108864
    
    # No port mapping needed with network_mode: host
    
    volumes:
      - ~/projects/vllm:/workspace/vllm
      - ~/projects/flashinfer:/workspace/flashinfer
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./.cache/vllm:/root/.cache/vllm
      - ./.cache/torch:/root/.cache/torch
      - ./.cache/triton:/root/.cache/triton
      - ./.cache/flashinfer:/root/.cache/flashinfer
      - ./.cache/uv:/root/.cache/uv
      - ./.cache/ccache:/root/.ccache
    
    environment:
      # CRITICAL: Required for FlashInfer CUTLASS MoE to work with editable install
      - PYTHONPATH=/workspace/flashinfer:/workspace/vllm
      - TIKTOKEN_ENCODINGS_BASE=/workspace/tiktoken_encodings
      
      # NOTE: Add vLLM runtime configuration here when needed
      # Example:
      # - VLLM_MXFP4_MOE_KERNEL=marlin
      # - VLLM_ATTENTION_SINKS=false
    
    # Serve GPT-OSS-120B with MXFP4
    command: >
      vllm serve openai/gpt-oss-120b
        --host 0.0.0.0
        --port 8000
        --served-model-name gpt-oss-120b
        --quantization mxfp4
        --mxfp4-backend CUTLASS
        --mxfp4-layers moe,qkv,o,lm_head
        --attention-backend FLASHINFER
        --kv-cache-dtype fp8
        --tensor-parallel-size 1
        --gpu-memory-utilization 0.70
        --max-model-len 131072
        --max-num-seqs 2
        --max-num-batched-tokens 8192
        --enable-prefix-caching
        --load-format fastsafetensors
    
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 300s

# Note: Using bind mounts for source code and caches

