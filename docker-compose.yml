# =============================================================================
# Docker Compose for GPT-OSS-120B with MXFP4 on DGX Spark (SM121/GB10)
# =============================================================================
#
# Achieves 59.4 tok/s decode with CUTLASS MXFP4 kernel.
#
# Usage:
#   docker compose build                 # Build the image
#   docker compose up -d                 # Start the service
#   docker compose logs -f               # View logs
#   docker compose down                  # Stop the service
#
# Benchmark:
#   docker compose exec vllm-mxfp4 llama-benchy \
#       --base-url http://localhost:8000/v1 \
#       --model gpt-oss-120b --pp 2048 --tg 32 128
#
# =============================================================================

services:
  vllm-mxfp4:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        BUILD_JOBS: 16
    image: vllm-mxfp4-spark:latest
    container_name: vllm-mxfp4
    restart: unless-stopped
    
    # GPU access
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    
    # Port mapping for vLLM API
    ports:
      - "8000:8000"
    
    # Shared memory for CUDA IPC
    ipc: host
    
    # Resource limits
    ulimits:
      memlock: -1
      stack: 67108864
    
    # Persistent volumes
    volumes:
      # HuggingFace model cache (required)
      - ~/.cache/huggingface:/root/.cache/huggingface
      # FlashInfer JIT cache (speeds up startup after first run)
      - ./.cache/flashinfer:/root/.cache/flashinfer
      # vLLM cache
      - ./.cache/vllm:/root/.cache/vllm
      # Ccache for rebuilds
      - ./.cache/ccache:/root/.ccache
    
    # Environment configuration
    environment:
      # MXFP4 Backend (CUTLASS for SM121)
      - VLLM_MXFP4_BACKEND=CUTLASS
      
      # Attention backend
      - VLLM_ATTENTION_BACKEND=FLASHINFER
      
      # FlashInfer settings
      - FLASHINFER_LOGLEVEL=0
      - FLASHINFER_JIT_VERBOSE=0
      - FLASHINFER_NVCC_THREADS=4
      
      # Tiktoken encodings
      - TIKTOKEN_ENCODINGS_BASE=/workspace/tiktoken_encodings
      
      # Optional: HuggingFace token for gated models
      # - HF_TOKEN=${HF_TOKEN}
      
      # Optional: Debug logging
      # - VLLM_MXFP4_DEBUG=1
    
    # Health check
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 300s

# =============================================================================
# Alternative configurations (uncomment to use)
# =============================================================================

  # Development mode - mounts local repos for live editing
  # vllm-dev:
  #   extends: vllm-mxfp4
  #   volumes:
  #     - ~/projects/vllm:/workspace/vllm
  #     - ~/projects/flashinfer:/workspace/flashinfer
  #   environment:
  #     - PYTHONPATH=/workspace/flashinfer:/workspace/vllm
