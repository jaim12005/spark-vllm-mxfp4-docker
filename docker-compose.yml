# =============================================================================
# Docker Compose for GPT-OSS-120B with MXFP4 on DGX Spark (SM121/GB10)
# =============================================================================
#
# Achieves 59.4 tok/s decode with CUTLASS MXFP4 kernel.
#
# Usage:
#   docker compose build                 # Build the image
#   docker compose up -d                 # Start the service
#   docker compose logs -f               # View logs
#   docker compose down                  # Stop the service
#
# Benchmark:
#   docker compose exec vllm-mxfp4 llama-benchy \
#       --base-url http://localhost:8000/v1 \
#       --model gpt-oss-120b --pp 2048 --tg 32 128
#
# =============================================================================

services:
  vllm-mxfp4:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        BUILD_JOBS: 16
    image: vllm-mxfp4-spark:latest
    container_name: vllm-mxfp4
    restart: unless-stopped
    
    # GPU access
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    
    # Port mapping
    ports:
      - "8000:8000"
    
    # Shared memory for CUDA IPC
    ipc: host
    
    # Resource limits
    ulimits:
      memlock: -1
      stack: 67108864
    
    # Persistent volumes
    volumes:
      # HuggingFace model cache (required)
      - ~/.cache/huggingface:/root/.cache/huggingface
      # FlashInfer JIT cache (speeds up startup after first run)
      - ./.cache/flashinfer:/root/.cache/flashinfer
      # vLLM cache
      - ./.cache/vllm:/root/.cache/vllm
      # Ccache for rebuilds
      - ./.cache/ccache:/root/.ccache
    
    # Environment configuration
    environment:
      # MXFP4 Backend
      - VLLM_MXFP4_BACKEND=CUTLASS
      
      # Attention backend
      - VLLM_ATTENTION_BACKEND=FLASHINFER
      
      # FlashInfer settings
      - FLASHINFER_LOGLEVEL=0
      - FLASHINFER_JIT_VERBOSE=0
      - FLASHINFER_NVCC_THREADS=4
      
      # Tiktoken encodings
      - TIKTOKEN_ENCODINGS_BASE=/workspace/tiktoken_encodings
      
      # Optional: HuggingFace token for gated models
      # - HF_TOKEN=${HF_TOKEN}
    
    # vLLM server command with optimized settings
    command: >
      vllm serve openai/gpt-oss-120b
        --host 0.0.0.0
        --port 8000
        --served-model-name gpt-oss-120b
        --quantization mxfp4
        --mxfp4-backend CUTLASS
        --mxfp4-layers moe,qkv,o,lm_head
        --attention-backend FLASHINFER
        --kv-cache-dtype fp8
        --tensor-parallel-size 1
        --gpu-memory-utilization 0.70
        --max-model-len 131072
        --max-num-seqs 2
        --max-num-batched-tokens 8192
        --enable-prefix-caching
        --load-format safetensors
    
    # Health check
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 300s

# =============================================================================
# Alternative configurations
# =============================================================================

  # Development mode - mounts local repos for live editing
  vllm-dev:
    extends:
      service: vllm-mxfp4
    container_name: vllm-dev
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./.cache/flashinfer:/root/.cache/flashinfer
      - ./.cache/vllm:/root/.cache/vllm
      - ./.cache/ccache:/root/.ccache
      # Mount local repos for development
      - ~/projects/vllm:/workspace/vllm
      - ~/projects/flashinfer:/workspace/flashinfer
    environment:
      - PYTHONPATH=/workspace/flashinfer:/workspace/vllm
      - VLLM_MXFP4_BACKEND=CUTLASS
      - VLLM_ATTENTION_BACKEND=FLASHINFER
      - FLASHINFER_LOGLEVEL=0
      - FLASHINFER_JIT_VERBOSE=0
      - FLASHINFER_NVCC_THREADS=4
      - TIKTOKEN_ENCODINGS_BASE=/workspace/tiktoken_encodings
    # Override command to just start bash for development
    command: ["bash"]
    stdin_open: true
    tty: true
    profiles:
      - dev
