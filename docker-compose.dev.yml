# =============================================================================
# Docker Compose for vLLM + FlashInfer Development on DGX Spark
# =============================================================================
#
# Usage:
#   docker compose -f docker-compose.dev.yml up -d    # Start dev container
#   docker compose -f docker-compose.dev.yml exec dev bash  # Shell into container
#   docker compose -f docker-compose.dev.yml down     # Stop container
#
# First run will install FlashInfer and vLLM in editable mode (takes a few minutes).
# Subsequent runs will be instant.
#
# =============================================================================

services:
  dev:
    build:
      context: .
      dockerfile: Dockerfile.dev
    image: vllm-dev:latest
    container_name: vllm-dev
    
    # GPU access
    gpus: all
    ipc: host
    
    # Keep container running for interactive use
    stdin_open: true
    tty: true
    
    # Resource limits
    ulimits:
      memlock: -1
      stack: 67108864
    
    # No ports - dev container is for interactive use
    # Use 'serve' service or run vllm manually with port mapping
    
    # Mount source code repos (EDIT THESE PATHS)
    volumes:
      # Source code mounts - your local repos
      - ~/projects/vllm:/workspace/vllm
      - ~/projects/flashinfer:/workspace/flashinfer
      
      # Scripts from this repo
      - ./scripts:/workspace/scripts
      
      # Persistent caches - survive container restarts
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./.cache/vllm:/root/.cache/vllm
      - ./.cache/torch:/root/.cache/torch
      - ./.cache/triton:/root/.cache/triton
      - ./.cache/flashinfer:/root/.cache/flashinfer
      - ./.cache/uv:/root/.cache/uv
      - ./.cache/ccache:/root/.ccache
      
      # Python site-packages cache (for editable installs to persist)
      - ./.cache/site-packages:/usr/local/lib/python3.12/dist-packages-dev
    
    # Environment variables
    environment:
      # CRITICAL: Required for FlashInfer CUTLASS MoE to work with editable install
      # This sets PYTHONPATH to include both repos. The ${PYTHONPATH:+:$PYTHONPATH} suffix
      # preserves any existing PYTHONPATH from the image (requires modern docker compose).
      - PYTHONPATH=/workspace/flashinfer:/workspace/vllm${PYTHONPATH:+:$PYTHONPATH}
      
      # vLLM PR branch for SM121/Blackwell support
      - VLLM_PR_NUMBER=31740
      - VLLM_REF=pr-31740
      
      # MXFP4 configuration
      - VLLM_USE_FLASHINFER_MOE_MXFP4_BF16=1
      - VLLM_USE_FLASHINFER_MOE_MXFP4_MXFP8=0
      - VLLM_USE_FLASHINFER_MOE_MXFP4_MXFP8_CUTLASS=0
      - VLLM_FLASHINFER_MOE_BACKEND=throughput
      - VLLM_USE_FLASHINFER_MOE_FP4=1
      
      # Performance
      - VLLM_ATTENTION_BACKEND=FLASHINFER
      - VLLM_USE_CUDA_GRAPH=1
      - FLASHINFER_NVCC_THREADS=4
      - FLASHINFER_CUDA_ARCH_LIST=12.1a
      
      # Development - enable verbose logging for debugging
      - FLASHINFER_LOGLEVEL=0
      - FLASHINFER_JIT_VERBOSE=0
      
      # Tiktoken encodings for tokenizer
      - TIKTOKEN_ENCODINGS_BASE=/workspace/tiktoken_encodings
      
      # Optional: HuggingFace token for gated models
      # - HF_TOKEN=${HF_TOKEN}
    
    # Default: start a shell
    command: /bin/bash
    
    # Working directory
    working_dir: /workspace

  # Optional: Run vLLM server as a separate service
  # Start with: docker compose -f docker-compose.dev.yml up serve
  serve:
    profiles:
      - serve
    build:
      context: .
      dockerfile: Dockerfile.dev
    image: vllm-dev:latest
    container_name: vllm-serve
    
    gpus: all
    ipc: host
    
    ulimits:
      memlock: -1
      stack: 67108864
    
    ports:
      - "8000:8000"
    
    volumes:
      - ~/projects/vllm:/workspace/vllm
      - ~/projects/flashinfer:/workspace/flashinfer
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./.cache/vllm:/root/.cache/vllm
      - ./.cache/torch:/root/.cache/torch
      - ./.cache/triton:/root/.cache/triton
      - ./.cache/flashinfer:/root/.cache/flashinfer
      - ./.cache/uv:/root/.cache/uv
      - ./.cache/ccache:/root/.ccache
    
    environment:
      # CRITICAL: Required for FlashInfer CUTLASS MoE to work with editable install
      - PYTHONPATH=/workspace/flashinfer:/workspace/vllm${PYTHONPATH:+:$PYTHONPATH}
      
      # vLLM PR branch for SM121/Blackwell support
      - VLLM_PR_NUMBER=31740
      - VLLM_REF=pr-31740
      
      - VLLM_USE_FLASHINFER_MOE_MXFP4_BF16=1
      - VLLM_USE_FLASHINFER_MOE_MXFP4_MXFP8=0
      - VLLM_USE_FLASHINFER_MOE_MXFP4_MXFP8_CUTLASS=0
      - VLLM_FLASHINFER_MOE_BACKEND=throughput
      - VLLM_USE_FLASHINFER_MOE_FP4=1
      - VLLM_ATTENTION_BACKEND=FLASHINFER
      - VLLM_USE_CUDA_GRAPH=1
      - FLASHINFER_CUDA_ARCH_LIST=12.1a
      - FLASHINFER_NVCC_THREADS=4
      - TIKTOKEN_ENCODINGS_BASE=/workspace/tiktoken_encodings
    
    # Serve GPT-OSS-120B with MXFP4
    # NOTE: --enforce-eager is required until CUDA graph capture crash is fixed
    command: >
      vllm serve openai/gpt-oss-120b
        --host 0.0.0.0
        --port 8000
        --served-model-name gpt-oss-120b
        --quantization mxfp4
        --tensor-parallel-size 1
        --gpu-memory-utilization 0.70
        --max-model-len 131072
        --max-num-seqs 2
        --max-num-batched-tokens 8192
        --enforce-eager
        --enable-prefix-caching
        --load-format fastsafetensors
    
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 300s

# Note: Using bind mounts for source code and caches

